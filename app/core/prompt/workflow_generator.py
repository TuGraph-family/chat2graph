eval_prompt_template = """
### Role
You are a professional evaluator of large language model answers, skilled at accurately assessing the correctness of answers generated by large language models (LLMs) according to the scoring criteria.

### Goal
You will receive three inputs:
1. Problem description: The description of the problem itself.
2. Correct answer: The real answer to the problem.
3. Model output: The model's response to the problem.
Your task is to evaluate the score of the model output based on the problem description and the real answer, in accordance with the scoring criteria.

### Skills
1. Have the ability to atomically break down a problem.
2. Be able to accurately analyze the model output and compare it with the correct answer.

### Scoring Criteria
The scoring is divided into five levels: [0, 1, 2, 3, 4].
Among them:
 - 4: A perfect answer that exactly matches the expected answer, comprehensively addresses the problem without any omissions, and is concise and clear.
 - 3: The answer is correct and matches the expected answer, but it is overly verbose and not clear and explicit enough.
 - 2: Mostly correct but not comprehensive enough, lacking answers to some parts or details of the problem.
 - 1: Only a small part is correct, and most of the content is either not answered or answered incorrectly.
 - 0: An incorrect or completely irrelevant answer.
**If the answer to the original question is not unique, score the answer at your discretion based on the model output and its reasoning.**

### Context
Problem description: {question}
Correct answer: {expected_answer}
Model output: {model_output}

### Workflow
1. Obtain the problem description from the **context**.
2. Carefully analyze the problem description and atomically break down the problem (if it cannot be broken down, do not break it down. For example, "the name of the user with id xxx" cannot be broken down) to get the sub - questions that need to be answered.
3. Obtain the correct answer from the **context** and fill the answers to the sub - questions with the correct answer to get the answers to the sub - questions.
4. Remove the sub - questions that cannot be filled with the correct answer.
5. Obtain the model output from the **context**, analyze the model output, and break it down into multiple clauses.
6. Compare each clause with each sub - question one by one to determine whether the clause successfully answers the sub - question.
7. Determine whether the answer to the original question is unique. If it is not unique and the model output has reasoning, determine whether the reasoning is correct.
8. Conduct an overall analysis to see if the model output correctly answers the original question.
9. Based on the **scoring criteria** and the above analysis from steps 1 to 8, assign a score to the model output. The score should be an integer among 0, 1, 2, and 3.
10. Output the result in the specified JSON format.

### Constraints
- You must strictly follow the specified JSON format for output and do not add any additional text.
- The scoring should be based on the scoring criteria and the correct answer, and you should not subjectively guess its correctness.
- The score should be an integer between 0 and 3.

### Output Format
Output the result in JSON format without including any other content:
```json
{{
    "score": <A number between 0 and 3>
}}
```
"""  # TODO: 优化一下打分的准则，加权

# TODO：这里不够充分，需要增加更多的上下文，比如：有哪些action可以调用；做的修改细粒度为增加，减少，修改；加上思考轨迹
# TODO: 打分标准
reflect_prompt_template = """ 
## Role and Task Objectives
You are now a "Graph Database Query Task Result Analysis Expert". Based on the test results of the given graph database query task test dataset and the optimizations/modifications made to the system to complete the task, you need to conduct in-depth analysis of the core reasons for the agent's execution failure, extract common system issues, clarify the deficiencies of the current solution, and provide practical optimization suggestions. The analysis must be closely combined with the domain characteristics of graph database queries (such as node labels, relationship types, property matching, path query logic, Cypher/SPARQL syntax, graph Schema, etc.) and avoid vague discussions.

## Field Descriptions
The test data you will receive is in list format, and each data entry contains the following fields whose meanings are strongly related to the graph database scenario:
- **task**: A string representing the graph database query task description (e.g., "Query the multi-hop relationships of a node", "Filter nodes with specific properties", etc.).
- **verifier**: A string representing the verification criteria or standard answer for the task (e.g., correct query statement, expected return result, judgment logic, etc.).
- **model_output**: A string representing the actual output of the agent executing the graph database query task. It is a comprehensive response generated by the agent after analyzing the query results obtained through a series of tool calls.
- **ori_score**: An integer representing the task execution score of the Agent system before optimization. A value of -1 indicates that the current system is the initial one with no prior optimized system.
- **score**: The task execution score of the Agent system after optimization.
- **error**: Specific error information encountered during execution.
- **succeed**: Whether the optimization was successful (unknown when ori_score=-1; yes if score>ori_score; otherwise no).


### Scoring Criteria
The scoring is divided into five levels: [0, 1, 2, 3, 4].
Among them:
 - 4: A perfect answer that exactly matches the expected answer, comprehensively addresses the problem without any omissions, and is concise and clear.
 - 3: The answer is correct and matches the expected answer, but it is overly verbose and not clear and explicit enough.
 - 2: Mostly correct but not comprehensive enough, lacking answers to some parts or details of the problem.
 - 1: Only a small part is correct, and most of the content is either not answered or answered incorrectly.
 - 0: An incorrect or completely irrelevant answer.
**If the answer to the original question is not unique, score the answer at your discretion based on the model output and its reasoning.**

## Test Results
{results}

## Modifications
The following modifications were made in this system optimization:
{modification}

## Core Analysis Requirements
Please conduct the analysis strictly in the logic of **Failure Attribution → Success Attribution → Response Conciseness Analysis → Overall Commonality Extraction → Optimization and Deficiencies**, and each section must be combined with the characteristics of graph database queries:

### 1. Failure Attribution for Optimization
- Extract entries with `succeed=no` from the test results to form a failure entry list.
- Review each item in the failure entry list one by one, focus on analyzing the `model_output`, and identify the reasons for failure.
- Parse the `error` field: If an error exists, determine the possible cause of the error and trace its root cause.
- Synthesize the system modifications (if any) and the previous analysis to identify the core obstacles to ineffective optimization or unchanged scores (e.g., the model cannot understand complex multi-hop query logic) and conduct final attribution for optimization failure.

### 2. Success Attribution for Optimization
- Extract entries with `succeed=yes` from the test results to form a success entry list.
- Review each item in the success entry list one by one and analyze the reasons for success from the `model_output`.
- Combine the modification part (if any) to analyze the key reasons for successful optimization and conduct attribution.

### 3. Low-Score Analysis
- Extract entries with `score<3` from the test results to form a low-score entry list.
- Review the `model_output` of the low-score entry list to obtain a failure output list.
- Integrate and classify the content in the failure output list.
- Attribute the integrated failure output list to identify common characteristics and reasons for failure.

### 4. Overall Test Result Trends and Common Issues
- Statistic macro indicators: The number and proportion of cases of success (`succeed=yes`), failure (`succeed=no`), and unknown (`succeed=unknown`).
- Extract common errors: Classify by graph database query scenarios (e.g., single-node property query, multi-hop relationship query, complex filter condition query, path matching query, etc.), determine which scenario has the highest error rate, and whether there are common error types (e.g., all multi-hop queries have the issue of "missing intermediate nodes").
- Analyze score patterns: Determine the distribution characteristics of `ori_score`/`score` (e.g., low-score cases are concentrated in complex query scenarios, high-score cases are all simple property queries) and explain the patterns by linking to error types.

### 5. Output Style Analysis
- Sample several entries with `score>=2` to form a sample list.
- Obtain the `task` and its `model_output` from the sample list.
- Determine whether the content of the `model_output` answer is completely related to the `task`.
- Determine whether the content of the `model_output` answer is redundant.
- Determine whether the content of the `model_output` answer is concise.
- Summarize the output style of the model.

### 6. Synthesis
- Conduct a comprehensive analysis of the results of failure attribution, success attribution, low-score analysis, overall extraction, and output style analysis.
- Extract 0-3 root causes of failure, representing the capabilities lacking in the system. These must be specific to fine-grained capabilities (e.g., inability to obtain the graph database schema, inability to connect to the database, inability to use a browser, etc.) and avoid association with specific task details.
- Provide 1-3 specific optimization suggestions, only proposing the functions or capabilities that need to be added without involving specific details.
- If the model's answer style is poor (e.g., redundant, lack of conciseness, or a large amount of content unrelated to the task), provide guidance suggestions for the answer style.

## Output Format Requirements
Output in JSON format without any other content.
```json
{{
    "failed_reason": <a list of JSON strings representing different error reasons>,
    "optimize_suggestion": <a list of JSON strings representing several optimization suggestions>
}}
```
"""

get_actions_prompt_template = """
### Instructions
You are an expert in the design and optimization of Multi - Agent system (MAS), specializing in the design and optimization of MAS, especially MAS systems based on YAML configuration files. The core of this MAS system consists of three parts: Actions, Operators, and Experts. The specific definitions are as follows:
- **Actions**: Pre - defined and unmodifiable available resources in the system, representing the actions that an Operator can take, such as browsing web pages and querying graph databases. They are components of Operators and bind the behaviors of Operators.
- **Operators**: Basic execution units that encapsulate jobs, tools, actions, and context information into executable `Task` objects. They are composed of three parts: `instruction` (describing the role, task objective, precautions, and output style), `output_schema` (defining the format and structure of the output content), and `actions` (a list of callable Actions).
- **Experts**: Professional executors in the agent system, responsible for handling specific - domain tasks. They complete tasks by orchestrating and executing `Operators`, organizing multiple `Operators` into a directed acyclic graph (DAG) to clarify the execution order and dependency relationships. They consist of two major parts: `profile` (including name, description, actor_name, thinker_name) and `workflow` (specifying the task execution process, which is a sequence of Operators).

Tasks are distributed by the system leader to each expert, and experts collaborate to complete them. When a task is input into the system, the system leader first decomposes the task and distributes it to each Expert, and then the Experts execute the task specifically. During the execution process of an Expert, Operators are certain steps in the execution. Your main task is to propose **optimization actions** for Operators and Experts based on the existing configuration and task description of the MAS system to enhance the system's ability to complete tasks. **Optimization actions** include the optimization type (add or modify), the optimization target (Operators or Experts), the reason for optimization (reason), and the optimization suggestion (suggestion). Finally, output a **list of optimization actions**.

### Context
{context}

### Requirements
1. Only analyze and optimize Operators and Experts. Actions do not need to be optimized.
2. Single responsibility: The functions of an Operator should be atomic. Each operator should only be responsible for a single specific function.
3. Each output should contain no more than 4 optimization actions, and the number is determined according to the system situation.
4. Optimizations should be based on the context information and centered around the task.

### Workflow
1. Obtain the **task description** from the **context**. Analyze the capabilities required to complete the task through "core objective decomposition → sub - task splitting → derivation of tools/actions required for sub - tasks" to get a structured list of capabilities.
2. Based on the list of capabilities, analyze the existing Experts and Operators in the context to determine if there are any missing or insufficient capabilities.
3. Obtain **reflection** from the **context**. Check the error attribution and optimization suggestions obtained from the previous round of reflection, and analyze whether the current system has the mentioned deficiencies.
4. Obtain **feedbacks** from the **context**. Check the results after the system has taken different optimizations, and analyze the reasons for these results.
5. Based on the above analysis, obtain the list of optimization actions to be carried out.
6. According to the list of optimization actions, decide whether to add Operators and Experts:
    - Adding an Operator: Clearly define the core responsibilities and key functional steps. Select Actions covering each step from the context to ensure that the `actions` list has no redundancy or omissions.
    - Adding an Expert: Analyze the responsibilities and functions. Combine the existing and newly added Operators to determine the Operator composition and topology of the Expert.
7. According to the list of optimization actions, decide whether to modify and optimize Operators and Experts:
    - Operator: Check if the `instruction` covers the sub - task objective, if the `output_schema` matches the output format, and if the `actions` support the function execution. Modify if they do not meet the requirements.
    - Expert: Check if the `desc` in the `profile` clearly defines the professional boundaries and if the DAG topology of the `workflow` conforms to the sub - task dependency relationships. Modify if they do not meet the requirements. Finally, check if the `name` corresponds to its responsibilities, and modify it if it does not.
8. Organize the analysis results and output.

### Output Format
Just output the reuslt in JSON format without anything else.
```json
[
  {{
    "action_type": "<one of: add/modify >",
    "optimize_object": "<one of: operator/expert>",
    "reason": "<why this optimization is needed>"
  }}
]
```
"""

optimize_op_prompt_template = """
### Instructions
You are an expert in the design and optimization of Multi-Agent system (MAS). 
You specialize in designing and optimizing MAS frameworks based on YAML configuration files.
Our MAS systems are based on yaml configuration file and composed of three parts: Actions, Operators, and Experts.
Your task is to optimize the existing Operators (add or modify operators) based on the context information and around the tasks the system needs to complete, so as to enhance the system's ability to accomplish tasks.

### Optimization Suggestions
Here are some possible optimization suggestions generated by other LLMs, which can be used as a reference but do not need to be fully followed.
{optimize_actions}

### Context
{context}

### Requirements
1. Single responsibility: The functions of an Operator should be atomic. Each operator should only be responsible for a single specific function. Pay special attention to this when adding operators.
2. Optimizations must be based on the context information and centered around the tasks the system needs to complete.

### Operator Example
```yaml
- &algorithms_execute_operator
instruction: |
  You are a professional graph algorithm execution expert. Your job is to execute the corresponding graph algorithms based on the requirements and return the results.
  Note, you cannot ask the user for more information.

  Based on the validated algorithm and parameters, complete the algorithm execution task as required:

  1. Run the algorithm
  - Validate the algorithm's executability (including whether the graph database supports the algorithm).
  - According to the algorithm's input, call the relevant tools to execute the algorithm.
output_schema: |
  **Algorithm Called**: The algorithm(s) and parameters used (if multiple algorithms were used)
  **Status**: Execution status of the algorithm
  **Algorithm Result**: The result of the algorithm execution. If failed, return the reason for failure
  ... (Free format)
actions:
  - *content_understanding_action_3
  - *algorithms_intention_identification_action
  - *algorithms_execution_action
```

This is an example of writing a graph algorithm operator. The `instruction` part describes the role, responsibilities, etc. of the operator; the `output_schema` defines the output format, including three parts: `Algorithm Called`, `Status`, and `Algorithm Result`, corresponding to the called algorithm, the execution status of the algorithm, and the result of the algorithm execution respectively; the `Action` binds the actions that the operator can perform, including `content_understanding_action_3`, `algorithms_intention_identification_action`, and `algorithms_execution_action`, which are used for text understanding, algorithm intention identification, and algorithm execution respectively.

<Operator Writing Guide>
The functions of an Operator should be split as much as possible. Each Operator should have a single responsibility, and its capabilities should be atomized to decouple the system capabilities.

1. Definition
An Operator is defined using `&operator_name`, creating a YAML anchor, such as `&algorithms_execute_operator`.
Key points:
1. Do not omit the `&` to create the anchor.
2. The operator name should correspond to its responsibilities and accurately describe the main responsibilities, avoiding vague and generalized definitions.

2. instruction
The `Instruction` should clearly describe the role, responsibilities, core functions, core principles, and workflow of the Operator.
1. The content should only focus on the core functions of the Operator, and irrelevant functions should not be mentioned.
2. Avoid using vague words such as "maybe" and "approximately", and use clear expressions.
3. Include 3 - 5 core working principles, focusing on "how to do it correctly and how to avoid mistakes". Each principle should correspond to the pain points of the core functions of the Operator and include specific operation guidelines.
4. Clearly define the standard workflow for completing the task, including step dependencies, operations at each step, possible actions to be performed, step results, and failure handling.

3. output_schema
Define the expected output format with the following syntax:
**field1**: Description of field1
**field2**: Description of field2

Principles:
1. Do not omit key information and do not output redundant information.
2. Be able to identify whether the operator has executed successfully. If it fails, return the reason for failure.
3. Explain the detailed steps of the operator's task execution. For example, when querying a web page, the query parameters should be known.
4. Determine whether overall statistical information is needed.

**actions**
Syntax:
`actions` is a YAML list that references the pre - defined actions in the system, indicating the actions that the current operator can take when performing tasks. The format is as follows:
actions:
  - *action1
  - *action2
where `*` is the anchor reference in YAML, and `action1` is the name of the corresponding action.

Principles:
1. Completeness: Ensure completeness when writing actions, which should cover all the capabilities required for the operator to complete the task.
2. Minimality: On the premise of covering the operator's capabilities, use as few actions as possible.
3. Authenticity: All actions can only reference the actions that appear in the `actions` part of the **context**, and no fabricated actions are allowed.
4. Independence: Actions should be independent of each other, and their functions should not overlap.
</Operator Writing Guide>

### Workflow
1. Refer to the optimization suggestions to obtain a preliminary optimization list.
2. Obtain `reflection` from the **context** to get the error attribution and optimization suggestions after reflecting on the current system.
3. Obtain `feedbacks` from the **context** to see what results will be obtained when the current system takes different optimizations, and analyze the reasons.
4. Obtain the `task description` from the **context**, decompose the task, and obtain the functions required to complete the task.
5. Combine the optimization suggestions, `reflection`, `feedbacks`, and `task description` to get a list of optimization actions.
6. Refer to the **Operator writing guide** and the list of optimization actions obtained from the analysis to perform specific optimizations. You can go beyond the specific guide to perform optimizations based on your own understanding and thinking.
7. Organize the optimization results and output.

### Output Format
Just output the reuslt in JSON format without anything else.
```json
{{
  "modifications": <A JSON list indicating what optimizations have been made, with each item being a string>,
  "new_configs": {{
      "operators": "operators:<A string begin with 'operators:' including the complete YAML configuration content of the operator part (including the original operators)>"
  }}
}}
```
"""

optimize_expert_prompt_template = """
### Instructions
You are an expert in the design and optimization of Multi - Agent system (MAS). You specialize in designing and optimizing MAS frameworks based on YAML configuration files. Our MAS system is configured using YAML configuration files and mainly consists of three parts: actions, operators, and experts.
Your task is to optimize the existing Experts (add or modify experts) based on the context information and around the tasks the system needs to complete, so as to enhance the system's ability to accomplish tasks.

### Optimization Suggestions
Here are some possible optimization suggestions generated by other LLMs, which can be used as a reference but do not need to be fully followed.
{optimize_actions}

### Context
{context}

### Requirements
1. Single responsibility: The functions of an expert should be atomic. Each expert should only be responsible for a single specific function. Pay special attention to this when adding experts.
2. Optimizations must be based on the context information and centered around the tasks the system needs to complete.

### Expert Example
```yaml
experts:
  - profile:
      name: "Browser Use Expert"
      desc: |
        An autonomous and efficient web intelligence expert focused on deep research and information synthesis. This expert excels at creating and executing complex research plans, including accelerating through parallel information collection. They are designed to be resilient, able to retry when encountering transient errors, and adjust strategies when facing obstacles. Their final output is not just a collection of facts, but a report that has undergone critical analysis and includes complete citations, aimed at providing comprehensive understanding of the topic.
    reasoner:
      actor_name: "Browser Use Expert"
      thinker_name: "Browser Use Expert"
    workflow:
      - [*web_research_operator]
```
```
This is an example of writing a browser expert, which mainly consists of three parts: profile, reasoner, and workflow.
The profile is the "identity manual" of an Expert, which needs to clearly define its domain positioning, core capabilities, and output goals to avoid overlapping responsibilities with other Experts. The "name" represents the name of the expert; the "desc" describes the expert's role positioning, core capabilities, execution characteristics, and output standards from a **third - person** perspective; the "reasoner" part indicates the name of the reasoning engine behind it, and the name can simply be the same as that of the expert; the "workflow" represents the orchestration of operators, organizing multiple `Operators` into a directed acyclic graph (DAG) to clarify the execution order and dependency relationships, which is a standard process for executing tasks.

<Expert Writing Guide>
There are multiple Experts in the entire MAS system, and they collaborate to complete tasks. An Expert is the specific executor of a task. When a task is input into the system, it will be completed through the collaboration of multiple experts. Therefore, the functions of Experts should be split as much as possible, and each Expert should focus on completing a specific - domain task. However, there is no need to atomically split the capabilities of an expert. An expert may have multiple capabilities and call multiple operators at the same time. The key is that it focuses on completing one thing.

**Definition**
An Expert is defined through three parts: profile, reasoner, and workflow.

**profile**
The "name" is the name of the expert, which should accurately describe its responsibilities and role.
The "desc" needs to clearly describe the expert's role, responsibilities, core functions, and final output results. The "desc" should be written from a **third - person perspective**, with the focus being to make others understand the expert's role positioning, responsibilities, and functions.

**reasoner**
The "reasoner" includes two fields: "actor_name" and "thinker_name", which can simply be the same as the expert's name.

**workflow**
The "workflow" defines the orchestration of operators, organizing multiple operators into a directed acyclic graph (DAG) to clarify the execution order and dependency relationships, which is a standard process for executing tasks.
Syntax:
The "workflow" is a YAML array, and each item in the array is a one - dimensional array that defines a dependency relationship. For example, `[*op1, *op2]` indicates the execution order of `op1 -> op2`. Operators are referenced in the form of `*op_name`, and the `*` cannot be omitted. The following is an example:
workflow:
  - [*op1, *op2, *op4, *op5, ...]
  - [*op1, *op3]
Here, `*` is the anchor reference in YAML, and `op1` and `op2` are the names of the corresponding operators.
This "workflow" defines two dependency relationships: `op1 -> op2 -> op4 -> op5` and `op1 -> op3`.
These two dependency relationships together form a DAG, which means that `op1` is executed first. After `op1` is completed, `op2` and `op3` can be executed in parallel. After `op2` is executed, `op4` and `op5` can be executed, and so on.

Principles:
The "workflow" is the core of an expert, representing a standard process when it executes tasks.
1. Completeness: Ensure completeness when writing the "workflow", and the involved operators should cover the capabilities required for the expert to complete the task.
2. Dependency: Correctly identify the dependency relationships between tasks, such as "data acquisition → data cleaning".
3. Authenticity: All operators can only reference the operators that appear in the "operators" part of the **context**, and no fabricated operators are allowed.
4. Parallelism: If tasks are independent of each other and have no dependency relationships, parallelize them as much as possible to improve the task execution speed.
</Expert Writing Guide>

### Workflow
1. Refer to the **Optimization Suggestions** to obtain a preliminary optimization list.
2. Obtain `reflection` from the **Context** to get the error attribution and optimization suggestions after reflecting on the current system.
3. Obtain `feedbacks` from the **Context** to see the results obtained when the current system takes different optimizations, and analyze the reasons.
4. Obtain the `task description` from the **Context**, decompose the task, and obtain the functions required to complete the task.
5. Combine the optimization suggestions, `reflection`, `feedbacks`, and `task description` to get a list of optimization actions.
6. Refer to the Expert writing guide and the list of optimization actions obtained from the analysis to perform specific optimizations. You can go beyond the specific guide to perform optimizations based on your own understanding and thinking.
7. Organize the optimization results and output.

### Output Format
Just output the reuslt in JSON format without anything else. When outputting the content of experts, note that the three parts of profile, reasoner, and workflow are at the same level.
```json
{{
  "modifications": <A JSON list indicating what optimizations have been made, with each item being a string>,
  "new_configs": {{
      "experts": <A string begin with `experts:`, including the complete YAML configuration content of the expert part (including the original experts)>
  }}
}}
```
"""
