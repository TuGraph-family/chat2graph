# After testing various public model services (considering factors such as hallucination, inference speed, cost, etc.),
# we recommend using the gemini 2.0 flash, gemini 2.5 flash, and o3-mini, or larger parameter LLMs for testing.
# Additionally, we recommend using DeepSeek V3 deployed on SiliconFlow, which offers advantages of easy access and low API pricing.

MODEL_PLATFORM_TYPE="LITELLM"  # Choose "LITELLM" or "AISUITE"

LLM_NAME=openai/deepseek-ai/DeepSeek-V3
LLM_ENDPOINT=https://api.siliconflow.cn/v1
LLM_APIKEY=

# Embedding model uses independent configuration system, not dependent on MODEL_PLATFORM_TYPE
# - Uses OpenAI compatible format by default, so no "openai/" prefix needed
# - Supports custom endpoints and API keys
EMBEDDING_MODEL_NAME=Qwen/Qwen3-Embedding-4B
EMBEDDING_MODEL_ENDPOINT=https://api.siliconflow.cn/v1/embeddings
EMBEDDING_MODEL_APIKEY=

TEMPERATURE=0
MAX_TOKENS=8192 # required by DeepSeek-V3
PRINT_REASONER_MESSAGES=1
PRINT_SYSTEM_PROMPT=1

LANGUAGE=en-US



### For users having issues with LLM service configuration: :)

# LiteLLM Configuration Rules (Recommended)
# LiteLLM is a unified LLM API interface supporting 100+ model providers
#
# How it works:
# ┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
# │  Chat2Graph     │───▶│    LiteLLM      │───▶│  Model Provider │
# │  Application    │    │    Router       │    │  (OpenAI/etc)   │
# └─────────────────┘    └─────────────────┘    └─────────────────┘
#                             │
#                             ▼
#                        Routes by model prefix
#
# Model name format (just some examples, model names may change):
# Format: "provider/model_name" or "provider/organization/model_name"
# - OpenAI Official: "openai/gpt-4o", "openai/gpt-3.5-turbo"
# - Anthropic Official: "anthropic/claude-3-5-sonnet-20240620"
# - Google Official: "gemini/gemini-2.5-pro", "gemini/gemini-2.0-flash"
# - Custom OpenAI Compatible: "openai/custom-model-name"
# - Third-party Platform: "openai/deepseek-ai/DeepSeek-V3" (SiliconFlow supports OpenAI compatible format by default)
#
# API endpoint routing logic:
# 1. LLM_ENDPOINT must always have a value
# 2. For official APIs, use the provider's official endpoint URL
# 3. For third-party platforms, use the platform's endpoint URL
# 4. LiteLLM automatically handles API format differences between providers
# 5. API key is set through LLM_APIKEY, or use environment variables (e.g., OPENAI_API_KEY)
#
# Example configurations:
# 
# Scenario 1: Using OpenAI Official API
# LLM_NAME=openai/gpt-4o
# LLM_ENDPOINT=https://api.openai.com/v1  # OpenAI official API endpoint
# LLM_APIKEY=sk-xxx  # OpenAI official API Key
#
# Scenario 2: Using third-party platform (e.g., SiliconFlow)
# LLM_NAME=openai/deepseek-ai/DeepSeek-V3  # Use OpenAI compatible format
# LLM_ENDPOINT=https://api.siliconflow.cn/v1  # Third-party platform endpoint
# LLM_APIKEY=sk-xxx  # Third-party platform API Key
#
# Scenario 3: Using Anthropic Official API
# LLM_NAME=anthropic/claude-3-5-sonnet-20240620
# LLM_ENDPOINT=https://api.anthropic.com  # Anthropic official API endpoint
# LLM_APIKEY=sk-ant-xxx  # Anthropic official API Key
#
# Scenario 4: Self-hosted OpenAI compatible service
# LLM_NAME=openai/your-model-name
# LLM_ENDPOINT=http://localhost:8000/v1  # Self-hosted service endpoint
# LLM_APIKEY=your-api-key  # Self-hosted service API Key
#
# Detailed request flow diagram:
# Chat2Graph Request
#       │
#       ▼
# ┌─────────────────────────────────────────────────────────────────┐
# │                      LiteLLM Router                             │
# │                                                                 │
# │  1. Parse model name prefix                                     │
# │     └─ "openai/" → OpenAI adapter                               │
# │     └─ "anthropic/" → Anthropic adapter                         │
# │     └─ "gemini/" → Google adapter                               │
# │                                                                 │
# │  2. Select corresponding provider adapter                       │
# │     └─ Set correct API format and parameters                    │
# │                                                                 │
# │  3. Handle API key & endpoint                                   │
# │     └─ LLM_ENDPOINT must always have a value                    │
# │     └─ Use official endpoint for official APIs                  │
# │     └─ Use custom endpoint for third-party/self-hosted          │
# │                                                                 │
# │  4. Format request parameters                                   │
# │     └─ Convert to target provider's API format                  │
# └─────────────────────────────────────────────────────────────────┘
#       │
#       ▼
# ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐
# │   OpenAI API    │  │ Anthropic API   │  │  Google API     │
# │                 │  │                 │  │                 │
# │Official/3rd/Self│  │   Official      │  │   Official      │
# └─────────────────┘  └─────────────────┘  └─────────────────┘
#
# Common troubleshooting:
# 1. Incorrect model name format → Check if prefix is correct (openai/, anthropic/, gemini/)
# 2. API key error → Confirm key format and permissions, check if expired
# 3. Endpoint unreachable → Check network connection and URL format, confirm endpoint is correct
# 4. Model not found → Confirm model name is available at provider, check spelling
# 5. Timeout error → Adjust network timeout settings, check provider service status
# 6. Quota limit → Check API quota and billing status
#
# Code implementation reference: lite_llm_client.py
#
#
# AISuite Configuration Rules (No longer updated)
# Note: AISuite project hasn't been updated for months, recommend using LiteLLM
#
# How it works:
# ┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
# │  Chat2Graph     │───▶│    AISuite      │───▶│  Model Provider │
# │  Application    │    │    Client       │    │  (OpenAI/etc)   │
# └─────────────────┘    └─────────────────┘    └─────────────────┘
#                             │
#                             ▼
#                        Hardcoded config routing
#
# Model name format:
# Format: "provider:model_name" (Note: use colon, not slash)
# - OpenAI: "openai:gpt-4o", "openai:gpt-3.5-turbo"
# - Anthropic: "anthropic:claude-3-5-sonnet-20240620"
# - Google: "google:gemini-pro"
# - Custom deployment: "openai:custom-model-name"
# - Third-party platform: "openai:deepseek-ai/DeepSeek-V3" (SiliconFlow supports OpenAI compatible format by default)
#
# Configuration limitations:
# - Does not support dynamic configuration via environment variables, poor flexibility
# - Can only support new providers or endpoints through code modification
#
# Code implementation reference: aisuite_client.py
