# After testing various public model services (considering factors such as hallucination, inference speed, cost, etc.),
# we recommend using the gemini 2.0 flash, gemini 2.5 flash, and o3-mini, or larger parameter LLMs for testing.
# Additionally, we recommend using DeepSeek V3 deployed on SiliconFlow, which offers advantages of easy access and low API pricing.

MODEL_PLATFORM_TYPE="LITELLM"  # default to "LITELLM"

# for more info about LLM and embedding models, please refer to doc: doc/en-us/deployment/config-env.md
LLM_NAME=openai/deepseek-ai/DeepSeek-V3
LLM_ENDPOINT=https://api.siliconflow.cn/v1
LLM_APIKEY=

EMBEDDING_MODEL_NAME=Qwen/Qwen3-Embedding-4B
EMBEDDING_MODEL_ENDPOINT=https://api.siliconflow.cn/v1/embeddings
EMBEDDING_MODEL_APIKEY=

TEMPERATURE=0
MAX_TOKENS=8192 # required by DeepSeek-V3
PRINT_REASONER_MESSAGES=1
PRINT_SYSTEM_PROMPT=1

LANGUAGE=en-US

# MemFuse memory integration (disabled by default)
# toggle MemFuse-backed memory. When disabled, Chat2Graph uses builtin memory only
ENABLE_MEMFUSE=false
PRINT_MEMORY_LOG=True
# MemFuse service endpoint and client behavior
MEMFUSE_BASE_URL=http://localhost:8765
MEMFUSE_TIMEOUT=30.0
MEMFUSE_RETRY_COUNT=3
# retrieval behavior
MEMFUSE_RETRIEVAL_TOP_K=5
